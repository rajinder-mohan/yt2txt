from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from typing import List, Optional, Union
import yt_dlp
import os
import tempfile
import shutil
import re
import time
import asyncio
from urllib.parse import unquote
from dotenv import load_dotenv
from deepgram import DeepgramClient
from openai import OpenAI
from database import (
    init_database,
    get_video_record,
    create_video_record,
    update_video_record,
    delete_audio_file_path,
    get_all_videos,
    get_stats,
    get_setting,
    set_setting,
    get_all_prompts,
    get_prompt,
    create_prompt,
    update_prompt,
    delete_prompt,
    create_generated_content,
    get_generated_content_by_video,
    get_generated_content,
    delete_generated_content,
    get_all_generated_content,
    update_video_ignored_status,
    bulk_update_video_ignored_status
)
from auth import authenticate_user, create_session, delete_session, get_current_user, authenticate_api_request, update_user_password, active_sessions
from email_service import send_channel_processing_results

# Load environment variables from .env file
load_dotenv()

from fastapi.security import HTTPBasic

# Configure Basic Auth for Swagger
basic_auth = HTTPBasic()

app = FastAPI(
    title="YouTube Audio Transcription Service", 
    version="1.0.0",
    description="""
    **YouTube Audio Transcription Service API**
    
    Complete API for downloading YouTube videos, transcribing audio, managing prompts, and generating AI content.
    
    ## Features
    
    - **Video Processing**: Download audio, extract metadata, transcribe videos
    - **Channel Management**: Extract videos from YouTube channels with pagination
    - **AI Integration**: Generate content using OpenAI GPT models
    - **Prompt Management**: Store and reuse AI prompts with variable templates
    - **Bulk Operations**: Process multiple videos simultaneously
    - **Content Storage**: Store transcripts and AI-generated content (1 video â†’ many content items)
    
    ## Authentication
    
    All API endpoints require authentication using one of these methods:
    - **Basic Auth**: Username and password (recommended for API calls)
    - **Bearer Token**: Session token from login endpoint (for dashboard sessions)
    
    ## Database
    
    Supports both PostgreSQL and SQLite databases. All large text fields (transcripts, prompts, generated content) use TEXT type for unlimited storage.
    
    ## Rate Limiting
    
    Built-in rate limiting protection for YouTube requests. Automatically handles bot detection and rate limit errors.
    """
)

def cookie_string_to_netscape(cookie_string: str, domain: str = ".youtube.com") -> str:
    """
    Convert cookie string (from browser) to Netscape format.
    
    Cookie string format: "name1=value1; name2=value2; ..."
    """
    import tempfile
    
    lines = [
        "# Netscape HTTP Cookie File",
        "# This file was generated by YouTube Transcription Service",
        "# This is a temporary file and will be cleaned up automatically",
        ""
    ]
    
    # Parse cookie string
    cookies = {}
    for cookie in cookie_string.split(';'):
        cookie = cookie.strip()
        if '=' in cookie:
            parts = cookie.split('=', 1)
            if len(parts) == 2:
                name = parts[0].strip()
                value = parts[1].strip()
                if name and value:
                    cookies[name] = value
    
    # Convert to Netscape format
    # Format: domain	flag	path	secure	expiration	name	value
    for name, value in cookies.items():
        # Netscape format: domain, flag, path, secure, expiration, name, value
        # expiration: 0 means session cookie, timestamp for persistent
        # Use 2147483647 (year 2038) as expiration for persistent cookies
        line = f"{domain}\tTRUE\t/\tTRUE\t2147483647\t{name}\t{value}"
        lines.append(line)
    
    # Create temporary file
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')
    temp_file.write('\n'.join(lines))
    temp_file.close()
    
    return temp_file.name


def get_ytdlp_options(base_opts: dict = None) -> dict:
    """
    Get yt-dlp options with cookie support and rate limiting if configured.
    
    Priority:
    1. Stored cookies from database (youtube_cookies setting)
    2. YOUTUBE_COOKIES_FILE: Path to cookies file (Netscape format)
    3. YOUTUBE_COOKIES_BROWSER: Browser to extract cookies from
    
    Rate limiting:
    - Adds sleep delay between requests to avoid YouTube rate limiting
    - Default: 2 seconds between requests
    - Configurable via YOUTUBE_SLEEP_INTERVAL environment variable
    """
    if base_opts is None:
        base_opts = {}
    
    # Add rate limiting (sleep between requests)
    # Default: 2 seconds, configurable via environment variable
    sleep_interval = float(os.getenv("YOUTUBE_SLEEP_INTERVAL", "2.0"))
    base_opts["sleep_interval"] = sleep_interval
    base_opts["sleep_interval_requests"] = 1  # Sleep after every request
    
    # First, check for stored cookies in database
    stored_cookies = get_setting("youtube_cookies")
    if stored_cookies:
        try:
            # Convert cookie string to Netscape format file
            cookies_file = cookie_string_to_netscape(stored_cookies)
            base_opts["cookies"] = cookies_file
            print(f"Using stored cookies from database (converted to: {cookies_file})")
            return base_opts
        except Exception as e:
            print(f"Warning: Failed to use stored cookies: {e}")
            import traceback
            traceback.print_exc()
    
    # Fallback to environment variables
    cookies_file = os.getenv("YOUTUBE_COOKIES_FILE")
    cookies_browser = os.getenv("YOUTUBE_COOKIES_BROWSER")
    
    # Add cookies if configured
    if cookies_file and os.path.exists(cookies_file):
        base_opts["cookies"] = cookies_file
    elif cookies_browser:
        # Support browser-based cookies (requires yt-dlp with browser support)
        base_opts["cookiesfrombrowser"] = (cookies_browser,)
    
    return base_opts

# Serve static files
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")

# Initialize database on startup
@app.on_event("startup")
async def startup_event():
    init_database()


class VideoRequest(BaseModel):
    """
    Request model for transcribing YouTube videos.
    
    You can provide videos in any of these ways:
    - videos: List of YouTube video IDs or URLs (recommended - accepts both)
    - video_ids: List of YouTube video IDs or URLs  
    - video_urls: List of YouTube video URLs
    
    Examples:
    - Video ID: "dQw4w9WgXcQ"
    - Full URL: "https://www.youtube.com/watch?v=dQw4w9WgXcQ"
    - Short URL: "https://youtu.be/dQw4w9WgXcQ"
    """
    videos: Optional[List[str]] = Field(
        None,
        description="List of YouTube video IDs or URLs (recommended - accepts both). Examples: 'dQw4w9WgXcQ', 'https://www.youtube.com/watch?v=VIDEO_ID', 'https://youtu.be/VIDEO_ID'",
        example=["dQw4w9WgXcQ", "https://www.youtube.com/watch?v=VIDEO_ID_2", "https://youtu.be/VIDEO_ID_3"]
    )
    video_ids: Optional[List[str]] = Field(
        None,
        description="List of YouTube video IDs or URLs. Examples: 'dQw4w9WgXcQ', 'https://www.youtube.com/watch?v=VIDEO_ID'",
        example=["dQw4w9WgXcQ", "https://www.youtube.com/watch?v=VIDEO_ID"]
    )
    video_urls: Optional[List[str]] = Field(
        None,
        description="List of YouTube video URLs. Examples: 'https://www.youtube.com/watch?v=VIDEO_ID', 'https://youtu.be/VIDEO_ID'",
        example=["https://www.youtube.com/watch?v=VIDEO_ID", "https://youtu.be/VIDEO_ID"]
    )
    deepgram_api_key: Optional[str] = Field(
        None,
        description="Deepgram API key (optional if set via DEEPGRAM_API_KEY environment variable)",
        example="your-deepgram-api-key"
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "videos": [
                    "dQw4w9WgXcQ",
                    "https://www.youtube.com/watch?v=VIDEO_ID_2",
                    "https://youtu.be/VIDEO_ID_3"
                ],
                "deepgram_api_key": "your-deepgram-api-key"
            }
        }


class TranscriptResponse(BaseModel):
    video_id: str
    video_url: Optional[str] = None
    transcript: str
    status: str
    from_cache: bool = False


class ErrorResponse(BaseModel):
    video_id: str
    video_url: Optional[str] = None
    error: str
    status: str


class TranscriptionResponse(BaseModel):
    success: List[TranscriptResponse]
    errors: List[ErrorResponse]


def extract_video_id(url_or_id: str) -> str:
    """
    Extract video ID from YouTube URL or return the ID if already provided.
    Supports various YouTube URL formats:
    - https://www.youtube.com/watch?v=VIDEO_ID
    - https://youtu.be/VIDEO_ID
    - https://www.youtube.com/embed/VIDEO_ID
    - https://www.youtube.com/v/VIDEO_ID
    - https://m.youtube.com/watch?v=VIDEO_ID
    - VIDEO_ID (direct ID)
    """
    # Strip whitespace
    url_or_id = url_or_id.strip()
    
    # If it's already just an ID (no special characters that URLs have)
    if not re.search(r'[?&=/]', url_or_id):
        return url_or_id
    
    # Try to extract from various YouTube URL formats
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/|youtube\.com\/v\/|m\.youtube\.com\/watch\?v=)([a-zA-Z0-9_-]{11})',
        r'youtube\.com\/watch\?.*[&?]v=([a-zA-Z0-9_-]{11})',
        r'youtu\.be\/([a-zA-Z0-9_-]{11})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url_or_id)
        if match:
            return match.group(1)
    
    # If no pattern matches, assume it's already an ID
    return url_or_id


def normalize_video_input(video_ids: Optional[List[str]], video_urls: Optional[List[str]], videos: Optional[List[str]] = None) -> List[tuple]:
    """
    Normalize video input to list of (video_id, video_url) tuples.
    Returns list of tuples: [(video_id, original_input), ...]
    Accepts video IDs, URLs, or a mix of both.
    """
    result = []
    
    # Handle the unified 'videos' field (accepts both IDs and URLs)
    if videos:
        for video in videos:
            video_id = extract_video_id(video)
            result.append((video_id, video))
    
    # Handle separate video_ids field (can contain IDs or URLs)
    if video_ids:
        for vid in video_ids:
            video_id = extract_video_id(vid)
            result.append((video_id, vid))
    
    # Handle separate video_urls field
    if video_urls:
        for url in video_urls:
            video_id = extract_video_id(url)
            result.append((video_id, url))
    
    # Remove duplicates while preserving order
    seen = set()
    unique_result = []
    for video_id, original in result:
        if video_id not in seen:
            seen.add(video_id)
            unique_result.append((video_id, original))
    
    return unique_result


def download_audio(video_id: str, output_dir: str) -> tuple[str, dict]:
    """
    Download audio from YouTube video and return the file path and metadata.
    
    Returns:
        tuple: (audio_file_path, metadata_dict)
    """
    url = f"https://www.youtube.com/watch?v={video_id}"
    os.makedirs(output_dir, exist_ok=True)

    base_opts = {
        "format": "bestaudio/best",
        "outtmpl": f"{output_dir}/{video_id}.%(ext)s",
        "postprocessors": [{
            "key": "FFmpegExtractAudio",
            "preferredcodec": "mp3",
            "preferredquality": "192",
        }],
        "noplaylist": True,
        "quiet": True,
        "no_warnings": True,
        # Add retry options for rate limiting
        "retries": 3,
        "fragment_retries": 3,
    }
    
    # Add cookie support and rate limiting if configured
    ydl_opts = get_ytdlp_options(base_opts)

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            # First extract info to get metadata
            # This can raise exceptions for rate limits or bot detection
            try:
                info = ydl.extract_info(url, download=False)
            except Exception as e:
                error_msg = str(e)
                # Check for rate limit or bot detection errors
                if ("Sign in to confirm you're not a bot" in error_msg or
                    "rate-limit" in error_msg.lower() or
                    "rate limit" in error_msg.lower() or
                    "rate-limited" in error_msg.lower() or
                    "429" in error_msg or
                    "too many requests" in error_msg.lower() or
                    "session has been rate-limited" in error_msg.lower() or
                    "premieres in" in error_msg.lower()):
                    # Re-raise as a special exception that will be caught and break the loop
                    raise Exception(f"RATE_LIMIT_ERROR: {error_msg}")
                # Re-raise other errors
                raise
            
            # Extract ALL available metadata from yt-dlp
            # Store everything we can get, not just common fields
            metadata = {}
            
            # Basic video info
            basic_fields = [
                'title', 'duration', 'view_count', 'upload_date', 'release_date', 
                'modified_date', 'availability', 'age_limit', 'description', 'thumbnail',
                'thumbnails', 'fps', 'width', 'height', 'resolution', 'format', 'format_id',
                'ext', 'filesize', 'filesize_approx', 'tbr', 'abr', 'acodec', 'vcodec',
                'container', 'protocol', 'format_note', 'language', 'language_preference'
            ]
            
            # Channel/Uploader info
            channel_fields = [
                'channel', 'channel_id', 'channel_url', 'channel_follower_count',
                'uploader', 'uploader_id', 'uploader_url', 'creator', 'artist'
            ]
            
            # Engagement metrics
            engagement_fields = [
                'like_count', 'dislike_count', 'comment_count', 'repost_count',
                'average_rating', 'playlist_count', 'playlist_id', 'playlist_title',
                'playlist_index', 'n_entries'
            ]
            
            # Content classification
            content_fields = [
                'tags', 'categories', 'genre', 'album', 'track', 'artist',
                'album_artist', 'release_year', 'release_month', 'release_day'
            ]
            
            # Subtitles and chapters
            subtitle_fields = [
                'subtitles', 'automatic_captions', 'chapters', 'chapter_count',
                'has_drm', 'is_live', 'live_status', 'was_live'
            ]
            
            # Additional metadata
            additional_fields = [
                'webpage_url', 'webpage_url_basename', 'original_url', 'display_id',
                'fulltitle', 'id', 'display_id', 'url', 'extractor', 'extractor_key',
                'epoch', 'timestamp', 'release_timestamp', 'modified_timestamp',
                'availability', 'license', 'concurrent_viewers', 'start_time', 'end_time'
            ]
            
            # Collect all fields
            all_fields = basic_fields + channel_fields + engagement_fields + content_fields + subtitle_fields + additional_fields
            
            for field in all_fields:
                value = info.get(field)
                if value is not None:
                    metadata[field] = value
            
            # Also store any other fields that might be present (catch-all)
            # This ensures we don't miss any metadata yt-dlp provides
            for key, value in info.items():
                if key not in metadata and value is not None:
                    # Skip internal yt-dlp fields that aren't useful
                    if not key.startswith('_') and key not in ['requested_formats', 'requested_subtitles', 'formats']:
                        try:
                            # Only store JSON-serializable values
                            import json
                            json.dumps(value)
                            metadata[key] = value
                        except (TypeError, ValueError):
                            # Skip non-serializable values
                            pass
            
            # Format upload date if available
            upload_date = info.get('upload_date')
            if upload_date and len(upload_date) == 8:
                formatted_date = f"{upload_date[:4]}-{upload_date[4:6]}-{upload_date[6:8]}"
            else:
                formatted_date = upload_date
            
            # Now download the audio
            # This can also raise exceptions for rate limits
            try:
                ydl.download([url])
            except Exception as e:
                error_msg = str(e)
                # Check for rate limit or bot detection errors
                if ("Sign in to confirm you're not a bot" in error_msg or
                    "rate-limit" in error_msg.lower() or
                    "rate limit" in error_msg.lower() or
                    "rate-limited" in error_msg.lower() or
                    "429" in error_msg or
                    "too many requests" in error_msg.lower() or
                    "session has been rate-limited" in error_msg.lower() or
                    "premieres in" in error_msg.lower()):
                    # Re-raise as a special exception that will be caught and break the loop
                    raise Exception(f"RATE_LIMIT_ERROR: {error_msg}")
                # Re-raise other errors
                raise
        
        # Find the downloaded file
        audio_file = None
        for ext in ['mp3', 'm4a', 'webm', 'opus']:
            potential_file = f"{output_dir}/{video_id}.{ext}"
            if os.path.exists(potential_file):
                audio_file = potential_file
                break
        
        if not audio_file:
            raise FileNotFoundError(f"Audio file not found for video {video_id}")
        
        # Return file path and metadata
        return audio_file, {
            'metadata': metadata,
            'title': metadata.get('title'),
            'duration': metadata.get('duration'),
            'view_count': metadata.get('view_count'),
            'upload_date': formatted_date,
            'channel_name': metadata.get('channel') or metadata.get('uploader'),
            'channel_id': metadata.get('channel_id') or metadata.get('uploader_id'),
        }
    except Exception as e:
        raise Exception(f"Failed to download audio: {str(e)}")


def transcribe_audio(audio_file_path: str, api_key: str) -> str:
    """Transcribe audio file using Deepgram API."""
    try:
        # Initialize Deepgram client with API key
        client = DeepgramClient(api_key=api_key)
        
        # Read audio file
        with open(audio_file_path, "rb") as audio_file:
            audio_data = audio_file.read()
        
        # Transcribe using the new API format
        response = client.listen.v1.media.transcribe_file(
            request=audio_data,
            model="nova-2",
            smart_format=True,
            language="en"
        )
        
        # Extract transcript text
        transcript = response.results.channels[0].alternatives[0].transcript
        
        return transcript
    except Exception as e:
        raise Exception(f"Failed to transcribe audio: {str(e)}")


@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_videos(request: VideoRequest):
    """
    **Transcribe YouTube Videos**
    
    Downloads audio from YouTube videos and transcribes them using Deepgram API.
    Stores transcripts in the database and returns the transcription results.
    
    **What it does:**
    1. Accepts one or more YouTube video IDs or URLs
    2. Downloads audio from each video using yt-dlp
    3. Extracts video metadata (title, channel, duration, etc.)
    4. Transcribes audio using Deepgram API
    5. Stores transcript and metadata in database
    6. Returns transcription results with full transcript text
    
    **Input formats:**
    - Video ID: "dQw4w9WgXcQ"
    - Full URL: "https://www.youtube.com/watch?v=dQw4w9WgXcQ"
    - Short URL: "https://youtu.be/dQw4w9WgXcQ"
    - Embed URL: "https://www.youtube.com/embed/dQw4w9WgXcQ"
    
    **Features:**
    - Duplicate detection: Skips already processed videos
    - Rate limiting: Handles YouTube rate limits gracefully
    - Metadata extraction: Stores all available video information
    - Error handling: Returns detailed error messages for failed videos
    
    **Response:**
    - Returns list of successful transcriptions with full transcript text
    - Returns list of errors with error messages
    - Each result includes video_id, video_url, transcript, and status
    """
    # Validate input
    if not request.videos and not request.video_ids and not request.video_urls:
        raise HTTPException(
            status_code=400,
            detail="At least one of 'videos', 'video_ids', or 'video_urls' must be provided"
        )
    
    # Get Deepgram API key
    api_key = request.deepgram_api_key or os.getenv("DEEPGRAM_API_KEY")
    if not api_key:
        raise HTTPException(
            status_code=400,
            detail="Deepgram API key is required. Provide it in the request or set DEEPGRAM_API_KEY environment variable."
        )
    
    # Normalize video input
    video_list = normalize_video_input(request.video_ids, request.video_urls, request.videos)
    
    # Create temporary directory for audio files
    temp_dir = tempfile.mkdtemp(prefix="youtube_audio_")
    
    success_results = []
    error_results = []
    
    # Get sleep interval for rate limiting
    sleep_interval = float(os.getenv("YOUTUBE_SLEEP_INTERVAL", "2.0"))
    
    for idx, (video_id, original_input) in enumerate(video_list):
        # Add delay between requests to avoid rate limiting (except for first video)
        if idx > 0:
            print(f"Rate limiting: Waiting {sleep_interval} seconds before processing next video...")
            await asyncio.sleep(sleep_interval)
        audio_file_path = None
        transcript = None
        from_cache = False
        
        try:
            # Check database for existing record
            db_record = get_video_record(video_id)
            
            if db_record:
                # Skip duplicate videos that are already processed
                if db_record['status'] in ['success', 'processed'] and db_record['transcript']:
                    print(f"Skipping duplicate video {video_id}: Already processed")
                    success_results.append(TranscriptResponse(
                        video_id=video_id,
                        video_url=db_record.get('video_url') or original_input,
                        transcript=db_record['transcript'],
                        status="success",
                        from_cache=True
                    ))
                    continue
                
                # Skip failed videos (don't re-download)
                if db_record['status'] == 'failed':
                    print(f"Skipping failed video {video_id}: Previous attempt failed")
                    error_results.append(ErrorResponse(
                        video_id=video_id,
                        video_url=db_record.get('video_url') or original_input,
                        error=db_record.get('error_message', 'Previous attempt failed'),
                        status="error"
                    ))
                    continue
                
                # Skip pending videos that are already in queue
                if db_record['status'] == 'pending':
                    print(f"Skipping pending video {video_id}: Already in queue")
                    # Still process it, but note it was pending
                
                # If status is pending or processing, continue with processing
                # If we have an audio file path from previous attempt, use it
                if db_record.get('audio_file_path') and os.path.exists(db_record['audio_file_path']):
                    audio_file_path = db_record['audio_file_path']
                    # Update status to processing
                    update_video_record(video_id, status="processing")
                else:
                    # Update record to processing
                    if db_record['status'] == 'pending':
                        update_video_record(video_id, status="processing", video_url=original_input)
                    else:
                        create_video_record(video_id, original_input, "processing")
                    # Download audio and get metadata
                    audio_file_path, video_metadata = download_audio(video_id, temp_dir)
                    update_video_record(
                        video_id, 
                        audio_file_path=audio_file_path,
                        title=video_metadata.get('title'),
                        duration=video_metadata.get('duration'),
                        view_count=video_metadata.get('view_count'),
                        upload_date=video_metadata.get('upload_date'),
                        channel_name=video_metadata.get('channel_name'),
                        channel_id=video_metadata.get('channel_id'),
                        metadata=video_metadata.get('metadata')
                    )
            else:
                # Create new record with processing status
                create_video_record(video_id, original_input, "processing")
                # Download audio and get metadata
                audio_file_path, video_metadata = download_audio(video_id, temp_dir)
                update_video_record(
                    video_id, 
                    audio_file_path=audio_file_path,
                    title=video_metadata.get('title'),
                    duration=video_metadata.get('duration'),
                    view_count=video_metadata.get('view_count'),
                    upload_date=video_metadata.get('upload_date'),
                    channel_name=video_metadata.get('channel_name'),
                    channel_id=video_metadata.get('channel_id'),
                    metadata=video_metadata.get('metadata')
                )
            
            # Transcribe audio
            transcript = transcribe_audio(audio_file_path, api_key)
            
            # Update database with processed status (success)
            update_video_record(
                video_id,
                status="processed",
                transcript=transcript
            )
            
            # Delete audio file only after successful transcription
            if audio_file_path and os.path.exists(audio_file_path):
                try:
                    os.remove(audio_file_path)
                    delete_audio_file_path(video_id)
                except Exception as e:
                    print(f"Warning: Could not delete audio file {audio_file_path}: {e}")
            
            success_results.append(TranscriptResponse(
                video_id=video_id,
                video_url=original_input,
                transcript=transcript,
                status="success",
                from_cache=False
            ))
            
        except Exception as e:
            error_msg = str(e)
            
            # Check if this is a rate limiting error
            # Also check for our special RATE_LIMIT_ERROR prefix
            is_rate_limit = (
                "RATE_LIMIT_ERROR:" in error_msg or
                "Sign in to confirm you're not a bot" in error_msg or
                "rate-limit" in error_msg.lower() or
                "rate limit" in error_msg.lower() or
                "rate-limited" in error_msg.lower() or
                "429" in error_msg or
                "too many requests" in error_msg.lower() or
                "session has been rate-limited" in error_msg.lower() or
                "premieres in" in error_msg.lower()  # Also catch "Premieres in X hours" errors
            )
            
            if is_rate_limit:
                # Mark as rate limited and stop processing
                print(f"\nâš ï¸ RATE LIMIT DETECTED for video {video_id}")
                print(f"Error: {error_msg}")
                
                # Mark current video as rate limited
                if db_record and db_record['status'] not in ['rate_limited', 'failed']:
                    update_video_record(
                        video_id,
                        status="rate_limited",
                        error_message=f"Rate limited: {error_msg}"
                    )
                elif not db_record:
                    create_video_record(video_id, original_input, "rate_limited")
                    update_video_record(
                        video_id,
                        error_message=f"Rate limited: {error_msg}"
                    )
                
                error_results.append(ErrorResponse(
                    video_id=video_id,
                    video_url=original_input,
                    error=f"Rate limited - processing stopped. Resume later to continue. Original error: {error_msg}",
                    status="error"
                ))
                
                # Break the loop - stop processing remaining videos
                remaining = len(video_list) - idx - 1
                print(f"\nðŸ›‘ Processing STOPPED due to rate limiting.")
                print(f"ðŸ“Š Progress: {idx + 1}/{len(video_list)} videos processed")
                print(f"â³ {remaining} videos remaining (will be skipped on next run)")
                print(f"ðŸ’¡ Next run will automatically skip already processed videos and resume from here.")
                break
            
            # Regular error handling (not rate limit)
            # Update database with failure status and reason
            if db_record and db_record['status'] != 'failed':
                update_video_record(
                    video_id,
                    status="failed",
                    error_message=error_msg
                )
            elif not db_record:
                create_video_record(video_id, original_input, "failed")
                update_video_record(
                    video_id,
                    error_message=error_msg
                )
            else:
                # Update existing failed record with new error message
                update_video_record(
                    video_id,
                    status="failed",
                    error_message=error_msg
                )
            
            # Don't delete audio file on failure - keep it for retry
            error_results.append(ErrorResponse(
                video_id=video_id,
                video_url=original_input,
                error=error_msg,
                status="error"
            ))
    
    # Clean up temp directory (only if empty or after processing)
    try:
        # Only remove if directory is empty or after a delay
        if os.path.exists(temp_dir):
            # Check if directory is empty
            if not os.listdir(temp_dir):
                shutil.rmtree(temp_dir)
            # If not empty, files are kept for failed attempts
    except Exception as e:
        print(f"Warning: Could not clean up temp directory: {e}")
    
    return TranscriptionResponse(
        success=success_results,
        errors=error_results
    )


@app.get("/video/{video_id}")
async def get_video_status(video_id: str):
    """
    **Get Video Information and Transcript**
    
    Retrieves complete information about a processed video including transcript, metadata, and status.
    
    **What it does:**
    - Looks up video in database by video ID or URL
    - Returns complete video data including:
      - Video ID and URL
      - Status (pending, processing, processed, failed, rate_limited)
      - Full transcript (if available)
      - Video metadata (title, channel, duration, view count, upload date)
      - Error messages (if processing failed)
      - Creation and update timestamps
    
    **Input:**
    - Video ID: `/video/dQw4w9WgXcQ`
    - Video URL: `/video/https://www.youtube.com/watch?v=dQw4w9WgXcQ`
    
    **Returns:**
    - 200: Complete video data with all fields
    - 404: Video not found in database (needs to be transcribed first)
    
    **Use cases:**
    - Check if a video has been processed
    - Retrieve transcript for a specific video
    - Get video metadata and statistics
    """
    import json
    
    # Decode URL-encoded path parameter
    decoded_id = unquote(video_id)
    
    # Extract video ID from URL if provided
    extracted_id = extract_video_id(decoded_id)
    
    db_record = get_video_record(extracted_id)
    
    if not db_record:
        raise HTTPException(
            status_code=404,
            detail=f"Video '{extracted_id}' not found in database. Please transcribe it first using POST /transcribe endpoint."
        )
    
    # Parse metadata if it's a JSON string
    metadata = db_record.get('metadata')
    if metadata:
        try:
            if isinstance(metadata, str):
                metadata = json.loads(metadata)
        except (json.JSONDecodeError, TypeError):
            metadata = None
    
    return {
        "video_id": db_record['video_id'],
        "video_url": db_record.get('video_url'),
        "status": db_record['status'],
        "transcript": db_record.get('transcript'),
        "error_message": db_record.get('error_message'),
        "title": db_record.get('title'),
        "duration": db_record.get('duration'),
        "view_count": db_record.get('view_count'),
        "upload_date": db_record.get('upload_date'),
        "channel_name": db_record.get('channel_name'),
        "channel_id": db_record.get('channel_id'),
        "metadata": metadata,
        "created_at": db_record['created_at'],
        "updated_at": db_record['updated_at']
    }


class ChannelRequest(BaseModel):
    """Request model for getting video IDs from a YouTube channel."""
    channel_url: str = Field(
        ...,
        description="YouTube channel URL. Examples: 'https://www.youtube.com/@channelname', 'https://www.youtube.com/channel/CHANNEL_ID', 'https://www.youtube.com/c/channelname', 'https://www.youtube.com/user/username'",
        example="https://www.youtube.com/@channelname"
    )
    max_results: Optional[int] = Field(
        None,
        description="Maximum number of videos to return (optional, returns all if not specified)",
        example=50
    )


class ChannelVideoInfo(BaseModel):
    """Information about a video from a channel."""
    video_id: str
    video_url: str
    title: str
    duration: Optional[int] = None
    view_count: Optional[int] = None
    upload_date: Optional[str] = None


class ChannelResponse(BaseModel):
    """Response model for channel video IDs."""
    channel_url: str
    channel_name: Optional[str] = None
    total_videos: int
    videos: List[ChannelVideoInfo]


def normalize_channel_url(channel_url: str) -> str:
    """
    Normalize channel URL to ensure we get the videos page (excluding shorts).
    If the URL is a channel URL without /videos, append it.
    """
    channel_url = channel_url.strip()
    
    # Remove /shorts if present - we don't want shorts
    if '/shorts' in channel_url:
        channel_url = channel_url.replace('/shorts', '/videos')
    
    # If it's already a videos page, return as is
    if '/videos' in channel_url or '/streams' in channel_url:
        return channel_url
    
    # If it's a channel URL, append /videos to get all videos (not shorts)
    if '@' in channel_url or '/channel/' in channel_url or '/c/' in channel_url or '/user/' in channel_url:
        # Remove trailing slash if present
        if channel_url.endswith('/'):
            channel_url = channel_url[:-1]
        # Append /videos if not already there
        if not channel_url.endswith('/videos'):
            channel_url = f"{channel_url}/videos"
    
    return channel_url


def extract_all_channel_videos(channel_url: str, max_results: Optional[int] = None, exclude_shorts: bool = True):
    """
    Extract all videos from a channel with pagination support.
    Excludes shorts by default.
    
    Returns:
        tuple: (videos_list, channel_name)
    """
    videos = []
    channel_name = None
    
    # Configure yt-dlp to extract all videos with pagination
    base_opts = {
        "quiet": True,
        "no_warnings": True,
        "extract_flat": False,  # Get full video info
        "playlistend": max_results if max_results else None,
        # Use extractor args to get all videos (not just recent) and exclude shorts
        "extractor_args": {
            "youtube": {
                "tab": "all",  # Get all videos, not just recent
            }
        },
        # Ignore shorts playlist
        "ignoreerrors": True,
    }
    
    # Add cookie support if configured
    ydl_opts = get_ytdlp_options(base_opts)
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        # Extract channel information with automatic pagination
        # process=True (default) ensures yt-dlp paginates through all videos
        info = ydl.extract_info(channel_url, download=False)
        
        # Get channel name
        channel_name = info.get('channel') or info.get('uploader') or info.get('channel_id')
        
        # Process entries (yt-dlp automatically handles pagination)
        if 'entries' in info:
            entries = info['entries']
            if entries:
                for entry in entries:
                    if entry is None:
                        continue
                    
                    video_id = entry.get('id')
                    if not video_id:
                        continue
                    
                    # Get video metadata
                    video_url = entry.get('url') or f"https://www.youtube.com/watch?v={video_id}"
                    title = entry.get('title', 'Unknown Title')
                    duration = entry.get('duration')
                    view_count = entry.get('view_count')
                    upload_date = entry.get('upload_date')
                    
                    # Skip shorts if enabled
                    if exclude_shorts:
                        # Check URL for shorts
                        if '/shorts/' in video_url or '/shorts' in video_url:
                            continue
                        
                        # Check title for shorts indicator
                        title_lower = title.lower()
                        if '#shorts' in title_lower or '#Shorts' in title_lower:
                            continue
                        
                        # Check duration (shorts are typically < 60 seconds)
                        if duration and duration < 60:
                            continue
                        
                        # Additional check: if duration is None, try to get it
                        if duration is None:
                            try:
                                video_info = ydl.extract_info(video_url, download=False)
                                duration = video_info.get('duration')
                                if duration and duration < 60:
                                    continue
                            except:
                                pass
                    
                    # Format upload date if available
                    if upload_date and len(upload_date) == 8:
                        formatted_date = f"{upload_date[:4]}-{upload_date[4:6]}-{upload_date[6:8]}"
                    else:
                        formatted_date = upload_date
                    
                    # Extract all available metadata from entry
                    video_metadata = {}
                    # Common fields
                    for field in ['title', 'duration', 'view_count', 'upload_date', 'description',
                                 'thumbnail', 'channel', 'channel_id', 'uploader', 'uploader_id',
                                 'like_count', 'comment_count', 'tags', 'categories', 'fps',
                                 'width', 'height', 'format', 'ext', 'filesize', 'language',
                                 'release_date', 'modified_date', 'availability', 'age_limit']:
                        value = entry.get(field)
                        if value is not None:
                            video_metadata[field] = value
                    
                    # Store any other available fields (catch-all for any additional metadata)
                    for key, value in entry.items():
                        if key not in video_metadata and value is not None and not key.startswith('_'):
                            try:
                                import json
                                json.dumps(value)
                                video_metadata[key] = value
                            except (TypeError, ValueError):
                                pass
                    
                    videos.append({
                        'video_id': video_id,
                        'video_url': video_url,
                        'title': title,
                        'duration': duration,
                        'view_count': view_count,
                        'upload_date': formatted_date,
                        'metadata': video_metadata  # Store ALL metadata
                    })
                    
                    # Stop if we've reached max_results
                    if max_results and len(videos) >= max_results:
                        break
    
    return videos, channel_name


@app.post("/channel/videos", response_model=ChannelResponse)
async def get_channel_videos(request: ChannelRequest):
    """
    **Extract Videos from YouTube Channel**
    
    Fetches all videos from a YouTube channel and stores their metadata in the database.
    
    **What it does:**
    1. Accepts a YouTube channel URL
    2. Extracts all videos from the channel using yt-dlp
    3. Automatically handles pagination to get ALL videos (not just recent)
    4. Excludes YouTube Shorts (videos < 60 seconds)
    5. Extracts video metadata (title, duration, view count, upload date)
    6. Stores videos in database with "pending" status
    7. Returns list of all found videos with metadata
    
    **Supported channel URL formats:**
    - `https://www.youtube.com/@channelname`
    - `https://www.youtube.com/channel/CHANNEL_ID`
    - `https://www.youtube.com/c/channelname`
    - `https://www.youtube.com/user/username`
    - `https://www.youtube.com/@channelname/videos`
    
    **Features:**
    - Automatic pagination: Gets all videos, not just first page
    - Shorts exclusion: Filters out videos shorter than 60 seconds
    - Metadata extraction: Captures title, duration, views, upload date
    - Duplicate prevention: Skips videos already in database
    - Stores videos as "pending" for later transcription
    
    **Response:**
    - Returns channel name and total video count
    - Returns list of videos with video_id, video_url, title, duration, view_count, upload_date
    """
    try:
        channel_url = normalize_channel_url(request.channel_url)
        max_results = request.max_results
        
        # Extract all videos (with pagination, excluding shorts)
        videos_data, channel_name = extract_all_channel_videos(
            channel_url, 
            max_results=max_results,
            exclude_shorts=True
        )
        
        # Convert to ChannelVideoInfo objects and store in database
        videos = []
        stored_count = 0
        
        for video_data in videos_data:
            video_id = video_data['video_id']
            video_url = video_data['video_url']
            title = video_data.get('title')
            duration = video_data.get('duration')
            view_count = video_data.get('view_count')
            upload_date = video_data.get('upload_date')
            
            # Get full metadata from video_data if available
            video_metadata = video_data.get('metadata', {})
            
            # Prepare metadata dict with all available data
            # Use metadata from video_data if available, otherwise use individual fields
            if video_metadata:
                metadata = video_metadata.copy()
                # Ensure we have the basic fields
                if title and 'title' not in metadata:
                    metadata['title'] = title
                if duration is not None and 'duration' not in metadata:
                    metadata['duration'] = duration
                if view_count is not None and 'view_count' not in metadata:
                    metadata['view_count'] = view_count
                if upload_date and 'upload_date' not in metadata:
                    metadata['upload_date'] = upload_date
                if channel_name and 'channel_name' not in metadata:
                    metadata['channel_name'] = channel_name
            else:
                # Fallback to individual fields
                metadata = {
                    'title': title,
                    'duration': duration,
                    'view_count': view_count,
                    'upload_date': upload_date,
                    'channel_name': channel_name,
                }
                # Remove None values from metadata
                metadata = {k: v for k, v in metadata.items() if v is not None}
            
            # Check if video already exists in database
            existing_record = get_video_record(video_id)
            
            if not existing_record:
                # Store new video with "pending" status and metadata
                create_video_record(
                    video_id, 
                    video_url, 
                    status="pending",
                    title=title,
                    duration=duration,
                    view_count=view_count,
                    upload_date=upload_date,
                    channel_name=channel_name,
                    metadata=metadata if metadata else None
                )
                stored_count += 1
            elif existing_record['status'] == 'pending':
                # Update URL and metadata if they changed
                update_video_record(
                    video_id, 
                    video_url=video_url,
                    title=title,
                    duration=duration,
                    view_count=view_count,
                    upload_date=upload_date,
                    channel_name=channel_name,
                    metadata=metadata if metadata else None
                )
            
            videos.append(ChannelVideoInfo(
                video_id=video_id,
                video_url=video_url,
                title=video_data['title'],
                duration=video_data.get('duration'),
                view_count=video_data.get('view_count'),
                upload_date=video_data.get('upload_date')
            ))
        
        if not videos:
            raise HTTPException(
                status_code=404,
                detail=f"No full-length videos found for channel URL: {channel_url} (shorts are excluded)"
            )
        
        return ChannelResponse(
            channel_url=channel_url,
            channel_name=channel_name,
            total_videos=len(videos),
            videos=videos
        )
        
    except yt_dlp.utils.DownloadError as e:
        error_msg = str(e)
        if "Private video" in error_msg or "Video unavailable" in error_msg:
            raise HTTPException(
                status_code=404,
                detail=f"Channel or videos not accessible: {error_msg}"
            )
        raise HTTPException(
            status_code=400,
            detail=f"Failed to extract videos from channel: {error_msg}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error processing channel URL: {str(e)}"
        )


@app.get("/channel/videos")
async def get_channel_videos_get(channel_url: str, max_results: Optional[int] = None):
    """
    **Extract Videos from YouTube Channel (GET)**
    
    Same functionality as POST /channel/videos but using GET method with query parameters.
    
    **What it does:**
    - Extracts all videos from a YouTube channel
    - Stores video metadata in database
    - Excludes shorts and handles pagination
    - Returns video list with metadata
    
    **Query Parameters:**
    - `channel_url` (required): YouTube channel URL
    - `max_results` (optional): Limit number of videos returned
    
    **See POST /channel/videos for detailed description.**
    """
    request = ChannelRequest(channel_url=channel_url, max_results=max_results)
    return await get_channel_videos(request)


# Authentication endpoints
class LoginRequest(BaseModel):
    username: str
    password: str


class LoginResponse(BaseModel):
    token: str
    username: str


@app.post("/api/login", response_model=LoginResponse)
async def login(request: LoginRequest):
    """
    **Admin Login**
    
    Authenticates admin user and returns a session token for API access.
    
    **What it does:**
    - Validates username and password
    - Creates a session token
    - Returns token for use in subsequent API requests
    
    **Response:**
    - Returns token and username on success
    - Returns 401 Unauthorized on invalid credentials
    
    **Usage:**
    - Use returned token in Authorization header: `Bearer <token>`
    - Token is valid for the session duration
    """
    if authenticate_user(request.username, request.password):
        token = create_session(request.username)
        return LoginResponse(token=token, username=request.username)
    raise HTTPException(status_code=401, detail="Invalid credentials")


class LogoutRequest(BaseModel):
    token: str


@app.post("/api/logout")
async def logout(request: LogoutRequest):
    """
    **Admin Logout**
    
    Invalidates a session token, logging out the user.
    
    **What it does:**
    - Deletes the session token from active sessions
    - Prevents further use of the token
    
    **Usage:**
    - Call this when user logs out to invalidate their session
    """
    delete_session(request.token)
    return {"message": "Logged out successfully"}


class ChangePasswordRequest(BaseModel):
    """Request model for changing password."""
    current_password: str
    new_password: str
    username: Optional[str] = Field(None, description="Username (optional, defaults to authenticated user)")


@app.post("/api/change-password")
async def change_password(
    request: ChangePasswordRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Change User Password**
    
    Changes the password for the authenticated user.
    
    **What it does:**
    - Verifies current password
    - Updates password hash in database
    - Invalidates all existing sessions (forces re-login)
    
    **Security:**
    - Requires current password verification
    - Invalidates all active sessions after password change
    - Requires authentication (Basic Auth or Bearer token)
    
    **Response:**
    - Returns success message
    - All active sessions are invalidated
    """
    # Use authenticated username (from Basic Auth or Bearer token)
    target_username = user
    
    # Verify current password
    if not authenticate_user(target_username, request.current_password):
        raise HTTPException(
            status_code=401,
            detail="Current password is incorrect"
        )
    
    # Validate new password
    if len(request.new_password) < 6:
        raise HTTPException(
            status_code=400,
            detail="New password must be at least 6 characters long"
        )
    
    # Update password
    if update_user_password(target_username, request.new_password):
        # Invalidate all sessions for this user (force re-login)
        tokens_to_delete = [token for token, username in active_sessions.items() if username == target_username]
        for token in tokens_to_delete:
            delete_session(token)
        
        return {"message": f"Password changed successfully for user '{target_username}'. Please login again."}
    else:
        raise HTTPException(
            status_code=500,
            detail="Failed to update password"
        )


# Admin dashboard endpoints
@app.get("/api/admin/videos")
async def get_all_videos_endpoint(user: str = Depends(authenticate_api_request)):
    """
    **Get All Videos (Admin)**
    
    Retrieves all videos from the database with complete information.
    
    **What it does:**
    - Returns all videos regardless of status
    - Includes complete metadata (title, channel, duration, views, etc.)
    - Includes transcripts if available
    - Returns error messages for failed videos
    - Ordered by most recently updated first
    
    **Use cases:**
    - Dashboard display of all videos
    - Complete video listing for admin interface
    - Export all video data
    
    **Note:** This endpoint returns ALL videos. Use GET /api/videos for filtered results.
    """
    return get_all_videos()


@app.get("/api/videos")
async def get_videos_endpoint(
    status: Optional[str] = None,
    channel: Optional[str] = None,
    search: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    limit: Optional[int] = None,
    offset: Optional[int] = None,
    show_ignored: Optional[bool] = False,
    user: str = Depends(authenticate_api_request)
):
    """
    **Get Videos with Advanced Filtering**
    
    Retrieves videos from the database with comprehensive filtering and pagination options.
    
    **What it does:**
    - Returns videos matching specified filters
    - Supports multiple filter combinations
    - Includes complete metadata for each video
    - Supports pagination for large result sets
    
    **Query Parameters:**
    - `status`: Filter by processing status
      - Options: `pending`, `processing`, `processed`, `failed`, `rate_limited`
      - `processed` includes both `success` and `processed` statuses
    - `channel`: Filter by exact channel/author name match
    - `search`: Search across video_id, title, and channel_name (partial match, case-insensitive)
    - `date_from`: Filter videos updated from this date (format: YYYY-MM-DD)
    - `date_to`: Filter videos updated until this date (format: YYYY-MM-DD)
    - `limit`: Maximum number of results to return
    - `offset`: Number of results to skip (for pagination)
    
    **Response:**
    - Returns total count and list of videos
    - Each video includes: id, video_id, video_url, status, transcript, title, duration, 
      view_count, upload_date, channel_name, channel_id, metadata, timestamps
    
    **Use cases:**
    - Filter videos by status for processing queue
    - Search for specific videos
    - Get videos from specific channel
    - Paginate through large video collections
    """
    from database import get_db_connection, DB_TYPE, fetch_all
    import json
    
    # Select all fields including metadata and ignored status
    query = """SELECT id, video_id, video_url, status, transcript, error_message, 
                      title, duration, view_count, upload_date, channel_name, channel_id, metadata,
                      ignored, created_at, updated_at 
               FROM video_transcriptions"""
    params = []
    conditions = []
    
    # Filter out ignored videos by default (unless show_ignored is True)
    if not show_ignored:
        if DB_TYPE == "postgres":
            conditions.append("(ignored IS NULL OR ignored = FALSE)")
        else:  # SQLite
            conditions.append("(ignored IS NULL OR ignored = 0)")
    
    if status:
        if status in ['pending', 'processing', 'processed', 'failed', 'success', 'rate_limited']:
            # Support both 'success' and 'processed' for backward compatibility
            if status == 'processed':
                conditions.append("status IN ('success', 'processed')")
            elif status == 'success':
                conditions.append("status IN ('success', 'processed')")
            else:
                conditions.append("status = %s" if DB_TYPE == "postgres" else "status = ?")
                params.append(status)
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid status. Must be one of: pending, processing, processed, failed, rate_limited"
            )
    
    if channel:
        conditions.append("channel_name = %s" if DB_TYPE == "postgres" else "channel_name = ?")
        params.append(channel)
    
    if search:
        search_pattern = f"%{search}%"
        if DB_TYPE == "postgres":
            conditions.append("(video_id ILIKE %s OR title ILIKE %s OR channel_name ILIKE %s)")
        else:
            conditions.append("(video_id LIKE ? OR title LIKE ? OR channel_name LIKE ?)")
        params.extend([search_pattern, search_pattern, search_pattern])
    
    if date_from:
        conditions.append("updated_at >= %s" if DB_TYPE == "postgres" else "updated_at >= ?")
        params.append(date_from)
    
    if date_to:
        conditions.append("updated_at <= %s" if DB_TYPE == "postgres" else "updated_at <= ?")
        params.append(f"{date_to} 23:59:59")
    
    if conditions:
        query += " WHERE " + " AND ".join(conditions)
    
    query += " ORDER BY updated_at DESC"
    
    if limit:
        query += " LIMIT %s" if DB_TYPE == "postgres" else " LIMIT ?"
        params.append(limit)
        
        if offset:
            query += " OFFSET %s" if DB_TYPE == "postgres" else " OFFSET ?"
            params.append(offset)
    
    with get_db_connection() as conn:
        videos = fetch_all(conn, query, tuple(params) if params else None)
    
    # Parse JSON metadata for each video
    for video in videos:
        if video.get('metadata'):
            try:
                if isinstance(video['metadata'], str):
                    video['metadata'] = json.loads(video['metadata'])
            except (json.JSONDecodeError, TypeError):
                video['metadata'] = None
    
    return {
        "total": len(videos),
        "videos": videos
    }


@app.get("/api/admin/stats")
async def get_stats_endpoint(user: str = Depends(authenticate_api_request)):
    """
    **Get Video Statistics**
    
    Returns aggregated statistics about videos in the database.
    
    **What it does:**
    - Counts videos by status
    - Provides overview of processing state
    
    **Response includes:**
    - `total`: Total number of videos
    - `success` / `processed`: Successfully processed videos
    - `failed`: Videos that failed processing
    - `processing`: Videos currently being processed
    - `pending`: Videos waiting to be processed
    - `rate_limited`: Videos blocked by YouTube rate limits
    
    **Use cases:**
    - Dashboard statistics display
    - Monitoring processing progress
    - Health checks and status overview
    """
    return get_stats()


# n8n Integration Endpoint for Channel Processing
class ChannelProcessRequest(BaseModel):
    """Request model for processing a YouTube channel (for n8n integration)."""
    channel_url: str = Field(..., description="YouTube channel URL")
    max_results: Optional[int] = Field(None, description="Maximum number of videos to process")
    deepgram_api_key: Optional[str] = Field(None, description="Deepgram API key (optional if set via env var)")


class ChannelProcessResponse(BaseModel):
    """Response model for channel processing."""
    channel_url: str
    channel_name: Optional[str] = None
    total_videos: int
    processed_videos: int
    success_count: int
    failed_count: int
    message: str
    video_results: List[dict]


@app.post("/api/channel/process", response_model=ChannelProcessResponse)
async def process_channel_videos(
    request: ChannelProcessRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Process Entire YouTube Channel**
    
    Complete workflow: Extracts all videos from a channel and transcribes them automatically.
    
    **What it does:**
    1. Extracts all videos from the channel URL (with pagination)
    2. Excludes YouTube Shorts (videos < 60 seconds)
    3. Stores video metadata in database
    4. Transcribes each video using Deepgram API
    5. Stores transcripts and results in database
    6. Handles rate limiting and errors gracefully
    7. Returns detailed results for each video
    
    **Features:**
    - Automatic pagination to get all videos
    - Skips already processed videos (duplicate detection)
    - Retries rate-limited videos on next run
    - Rate limiting between requests to avoid YouTube blocks
    - Comprehensive error handling
    
    **Response:**
    - Returns channel information
    - Returns total videos found
    - Returns count of successful and failed transcriptions
    - Returns detailed results for each video (status, transcript, error)
    
    **Use cases:**
    - n8n workflow integration
    - Batch processing entire channels
    - Automated transcription pipeline
    """
    try:
        # Step 1: Get video IDs from channel (with pagination, excluding shorts)
        channel_url = normalize_channel_url(request.channel_url)
        max_results = request.max_results
        
        # Extract all videos (excluding shorts)
        videos_data, channel_name = extract_all_channel_videos(
            channel_url,
            max_results=max_results,
            exclude_shorts=True
        )
        
        videos_info = [
            {
                'video_id': video['video_id'],
                'video_url': video['video_url'],
                'title': video['title']
            }
            for video in videos_data
        ]
        
        if not videos_info:
            raise HTTPException(
                status_code=404,
                detail=f"No full-length videos found for channel URL: {channel_url} (shorts are excluded)"
            )
        
        # Step 2: Transcribe all videos
        api_key = request.deepgram_api_key or os.getenv("DEEPGRAM_API_KEY")
        if not api_key:
            raise HTTPException(
                status_code=400,
                detail="Deepgram API key is required. Provide it in the request or set DEEPGRAM_API_KEY environment variable."
            )
        
        # Create video request for transcription
        video_list = [(v['video_id'], v['video_url']) for v in videos_info]
        temp_dir = tempfile.mkdtemp(prefix="youtube_audio_")
        
        success_count = 0
        failed_count = 0
        skipped_count = 0
        video_results = []
        
        # Get sleep interval for rate limiting
        sleep_interval = float(os.getenv("YOUTUBE_SLEEP_INTERVAL", "2.0"))
        
        for idx, (video_id, video_url) in enumerate(video_list):
            # Add delay between requests to avoid rate limiting (except for first video)
            if idx > 0:
                print(f"Rate limiting: Waiting {sleep_interval} seconds before processing next video...")
                await asyncio.sleep(sleep_interval)
            audio_file_path = None
            transcript = None
            error_msg = None
            status = "failed"
            
            try:
                # Check database for existing record
                db_record = get_video_record(video_id)
                
                if db_record and db_record['status'] in ['success', 'processed'] and db_record['transcript']:
                    # Skip already processed videos (duplicate detection)
                    transcript = db_record['transcript']
                    status = "processed"
                    success_count += 1
                    skipped_count += 1
                    print(f"Skipping video {video_id}: Already processed")
                elif db_record and db_record['status'] == 'failed':
                    # Skip failed videos (don't retry automatically)
                    error_msg = db_record.get('error_message', 'Previous attempt failed')
                    failed_count += 1
                    skipped_count += 1
                    print(f"Skipping video {video_id}: Previous attempt failed")
                elif db_record and db_record['status'] == 'pending':
                    # Skip pending videos that haven't been processed yet (will be processed separately)
                    status = "pending"
                    skipped_count += 1
                    print(f"Skipping video {video_id}: Already in queue (pending)")
                elif db_record and db_record['status'] == 'rate_limited':
                    # Resume from rate-limited videos (retry them)
                    print(f"ðŸ”„ Resuming rate-limited video {video_id}: Retrying...")
                    # Update status to processing and continue
                    update_video_record(video_id, status="processing")
                    # Fall through to process the video
                
                # Process video (new, pending, rate_limited that we're retrying, or processing)
                if not db_record or db_record['status'] in ['pending', 'rate_limited', 'processing']:
                    # Process video
                    if not db_record:
                        create_video_record(video_id, video_url, "processing")
                    
                    # Download audio and get metadata
                    if db_record and db_record.get('audio_file_path') and os.path.exists(db_record['audio_file_path']):
                        audio_file_path = db_record['audio_file_path']
                        # Metadata already stored, skip re-extraction
                    else:
                        audio_file_path, video_metadata = download_audio(video_id, temp_dir)
                        update_video_record(
                            video_id, 
                            audio_file_path=audio_file_path,
                            title=video_metadata.get('title'),
                            duration=video_metadata.get('duration'),
                            view_count=video_metadata.get('view_count'),
                            upload_date=video_metadata.get('upload_date'),
                            channel_name=video_metadata.get('channel_name'),
                            channel_id=video_metadata.get('channel_id'),
                            metadata=video_metadata.get('metadata')
                        )
                    
                    # Transcribe
                    transcript = transcribe_audio(audio_file_path, api_key)
                    update_video_record(video_id, status="processed", transcript=transcript)
                    
                    # Delete audio file
                    if audio_file_path and os.path.exists(audio_file_path):
                        try:
                            os.remove(audio_file_path)
                            delete_audio_file_path(video_id)
                        except Exception as e:
                            print(f"Warning: Could not delete audio file {audio_file_path}: {e}")
                    
                    status = "processed"
                    success_count += 1
                
            except Exception as e:
                error_msg = str(e)
                
                # Check if this is a rate limiting error
                # Also check for our special RATE_LIMIT_ERROR prefix
                is_rate_limit = (
                    "RATE_LIMIT_ERROR:" in error_msg or
                    "Sign in to confirm you're not a bot" in error_msg or
                    "rate-limit" in error_msg.lower() or
                    "rate limit" in error_msg.lower() or
                    "rate-limited" in error_msg.lower() or
                    "429" in error_msg or
                    "too many requests" in error_msg.lower() or
                    "session has been rate-limited" in error_msg.lower() or
                    "premieres in" in error_msg.lower()  # Also catch "Premieres in X hours" errors
                )
                
                if is_rate_limit:
                    # Mark as rate limited and stop processing
                    print(f"âš ï¸ RATE LIMIT DETECTED for video {video_id}. Stopping processing.")
                    print(f"Error: {error_msg}")
                    
                    # Mark current video as rate limited
                    db_record = get_video_record(video_id)
                    if db_record and db_record['status'] not in ['rate_limited', 'failed']:
                        update_video_record(video_id, status="rate_limited", error_message=f"Rate limited: {error_msg}")
                    elif not db_record:
                        create_video_record(video_id, video_url, "rate_limited")
                        update_video_record(video_id, error_message=f"Rate limited: {error_msg}")
                    
                    status = "rate_limited"
                    failed_count += 1
                    
                    # Break the loop - stop processing remaining videos
                    remaining = len(video_list) - idx - 1
                    print(f"ðŸ›‘ Processing stopped due to rate limiting. {remaining} videos remaining.")
                    print(f"ðŸ’¡ Next run will automatically skip already processed videos and resume from here.")
                    break
                
                # Regular error handling (not rate limit)
                status = "failed"
                failed_count += 1
                
                # Update database with failure
                db_record = get_video_record(video_id)
                if db_record and db_record['status'] != 'failed':
                    update_video_record(video_id, status="failed", error_message=error_msg)
                elif not db_record:
                    create_video_record(video_id, video_url, "failed")
                    update_video_record(video_id, error_message=error_msg)
            
            # Find video title
            video_title = next((v['title'] for v in videos_info if v['video_id'] == video_id), 'Unknown Title')
            
            video_results.append({
                'video_id': video_id,
                'video_url': video_url,
                'title': video_title,
                'status': status,
                'transcript': transcript if status in ['success', 'processed'] else None,
                'error': error_msg if status == 'failed' else None
            })
        
        # Clean up temp directory
        try:
            if os.path.exists(temp_dir) and not os.listdir(temp_dir):
                shutil.rmtree(temp_dir)
        except Exception as e:
            print(f"Warning: Could not clean up temp directory: {e}")
        
        total_processed = success_count + failed_count
        message = f"Processed {len(videos_info)} videos. {success_count} successful ({skipped_count} skipped/duplicates), {failed_count} failed."
        
        return ChannelProcessResponse(
            channel_url=channel_url,
            channel_name=channel_name,
            total_videos=len(videos_info),
            processed_videos=len(video_results),
            success_count=success_count,
            failed_count=failed_count,
            message=message,
            video_results=video_results
        )
        
    except yt_dlp.utils.DownloadError as e:
        error_msg = str(e)
        if "Private video" in error_msg or "Video unavailable" in error_msg:
            raise HTTPException(
                status_code=404,
                detail=f"Channel or videos not accessible: {error_msg}"
            )
        raise HTTPException(
            status_code=400,
            detail=f"Failed to extract videos from channel: {error_msg}"
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error processing channel: {str(e)}"
        )


# Admin dashboard GUI
@app.get("/", response_class=HTMLResponse)
async def admin_dashboard():
    """
    **Admin Dashboard (Web UI)**
    
    Serves the admin dashboard HTML page.
    
    **What it does:**
    - Returns the admin dashboard interface
    - Provides web UI for all operations
    - No authentication required (handled by frontend)
    
    **Features:**
    - Video management and viewing
    - Transcript viewing
    - Channel processing
    - AI content generation
    - Prompt management
    - User management
    - Settings configuration
    
    **Access:**
    - Navigate to root URL (/) in browser
    - Login required to access features
    """
    return FileResponse("static/admin.html")


@app.get("/health")
async def health():
    """
    **Health Check**
    
    Simple health check endpoint to verify API is running.
    
    **What it does:**
    - Returns API status
    - No authentication required
    - Used for monitoring and load balancers
    
    **Response:**
    - Returns {"status": "healthy"} if API is running
    
    **Use cases:**
    - Health monitoring
    - Load balancer checks
    - Service availability verification
    """
    return {"status": "healthy"}


# User Management Endpoints
class CreateUserRequest(BaseModel):
    """Request model for creating a user."""
    username: str = Field(..., min_length=3, description="Username (minimum 3 characters)")
    password: str = Field(..., min_length=6, description="Password (minimum 6 characters)")


class DeleteUserRequest(BaseModel):
    """Request model for deleting a user."""
    username: str


class UserInfo(BaseModel):
    """User information model."""
    username: str
    created_at: Optional[str] = None


@app.get("/api/admin/users", response_model=List[UserInfo])
async def list_users(user: str = Depends(authenticate_api_request)):
    """
    **List All Admin Users**
    
    Retrieves a list of all admin users in the system.
    
    **What it does:**
    - Returns all registered admin users
    - Includes username and creation date
    - Ordered by creation date
    
    **Response:**
    - Returns list of users with: username, created_at
    
    **Use cases:**
    - Display user list in admin dashboard
    - User management interface
    - Audit user accounts
    """
    from database import get_db_connection, DB_TYPE
    
    try:
        with get_db_connection() as conn:
            if DB_TYPE == "postgres":
                from psycopg2.extras import RealDictCursor
                cursor = conn.cursor(cursor_factory=RealDictCursor)
                cursor.execute("SELECT username, created_at FROM admin_users ORDER BY created_at")
                rows = cursor.fetchall()
                cursor.close()
                users = [{"username": row['username'], "created_at": str(row.get('created_at', ''))} for row in rows]
            else:  # SQLite
                cursor = conn.execute("SELECT username, created_at FROM admin_users ORDER BY created_at")
                rows = cursor.fetchall()
                users = [{"username": row['username'], "created_at": row['created_at']} for row in rows]
        
        return users
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to list users: {str(e)}"
        )


@app.post("/api/admin/users", response_model=UserInfo)
async def create_user(request: CreateUserRequest, user: str = Depends(authenticate_api_request)):
    """
    **Create Admin User**
    
    Creates a new admin user account with username and password.
    
    **What it does:**
    - Creates new user account
    - Hashes password securely (SHA256)
    - Stores user in database
    - Updates existing user if username already exists (password update)
    
    **Required Fields:**
    - `username`: Minimum 3 characters
    - `password`: Minimum 6 characters
    
    **Response:**
    - Returns created user info with username
    - Returns 400 if username already exists (unless updating password)
    - Returns 500 on creation failure
    
    **Use cases:**
    - Add new admin users
    - Create team accounts
    - Reset user passwords
    """
    from auth import hash_password
    from database import get_db_connection, DB_TYPE
    
    try:
        password_hash = hash_password(request.password)
        
        with get_db_connection() as conn:
            if DB_TYPE == "postgres":
                cursor = conn.cursor()
                try:
                    cursor.execute("""
                        INSERT INTO admin_users (username, password_hash)
                        VALUES (%s, %s)
                        ON CONFLICT (username) DO UPDATE 
                        SET password_hash = EXCLUDED.password_hash
                    """, (request.username, password_hash))
                    cursor.close()
                except Exception as e:
                    cursor.close()
                    raise e
            else:  # SQLite
                conn.execute("""
                    INSERT OR REPLACE INTO admin_users (username, password_hash)
                    VALUES (?, ?)
                """, (request.username, password_hash))
        
        return {
            "username": request.username,
            "created_at": None
        }
    except Exception as e:
        error_msg = str(e)
        if "UNIQUE constraint" in error_msg or "duplicate key" in error_msg.lower():
            raise HTTPException(
                status_code=400,
                detail=f"Username '{request.username}' already exists"
            )
        raise HTTPException(
            status_code=500,
            detail=f"Failed to create user: {str(e)}"
        )


@app.delete("/api/admin/users/{username}")
async def delete_user(username: str, user: str = Depends(authenticate_api_request)):
    """
    **Delete Admin User**
    
    Permanently deletes an admin user account from the system.
    
    **What it does:**
    - Removes user from database
    - Prevents self-deletion (security measure)
    - Permanent action (cannot be undone)
    
    **Security:**
    - Cannot delete your own account
    - Requires authentication
    - Returns 400 if attempting self-deletion
    
    **Response:**
    - Returns 200 with success message if deleted
    - Returns 404 if user not found
    - Returns 400 if attempting to delete yourself
    - Returns 500 on deletion failure
    
    **Use cases:**
    - Remove user accounts
    - Clean up inactive users
    - User management
    """
    from database import get_db_connection, DB_TYPE
    
    # Prevent deleting yourself
    if username == user:
        raise HTTPException(
            status_code=400,
            detail="You cannot delete your own account"
        )
    
    try:
        with get_db_connection() as conn:
            if DB_TYPE == "postgres":
                cursor = conn.cursor()
                cursor.execute("DELETE FROM admin_users WHERE username = %s", (username,))
                deleted_count = cursor.rowcount
                cursor.close()
            else:  # SQLite
                cursor = conn.execute("DELETE FROM admin_users WHERE username = ?", (username,))
                deleted_count = cursor.rowcount
        
        if deleted_count == 0:
            raise HTTPException(
                status_code=404,
                detail=f"User '{username}' not found"
            )
        
        return {"message": f"User '{username}' deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete user: {str(e)}"
        )


# Settings Management Endpoints
class SettingRequest(BaseModel):
    """Request model for updating a setting."""
    value: str


@app.get("/api/admin/settings/{setting_key}")
async def get_setting_endpoint(setting_key: str, user: str = Depends(authenticate_api_request)):
    """
    **Get Setting Value**
    
    Retrieves the value of a specific setting from the database.
    
    **What it does:**
    - Looks up setting by key
    - Returns setting value (if exists)
    - Returns null if setting not found
    
    **Common Setting Keys:**
    - `n8n_webhook_url`: n8n webhook URL for channel processing
    - `youtube_cookies`: YouTube cookies for bot detection bypass
    - `use_webhook`: Enable/disable webhook integration
    
    **Response:**
    - Returns setting_key and value
    - Value is null if setting doesn't exist
    
    **Use cases:**
    - Retrieve configuration values
    - Check if settings are configured
    - Settings management
    """
    value = get_setting(setting_key)
    return {"setting_key": setting_key, "value": value}


@app.post("/api/admin/settings/{setting_key}")
async def set_setting_endpoint(
    setting_key: str,
    request: SettingRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Set Setting Value**
    
    Creates or updates a setting value in the database.
    
    **What it does:**
    - Stores key-value pair in settings table
    - Updates existing setting if key already exists
    - Updates timestamp automatically
    
    **Common Settings:**
    - `n8n_webhook_url`: Set n8n webhook URL
    - `youtube_cookies`: Store YouTube cookies
    - `use_webhook`: Enable/disable webhook (value: "true" or "false")
    
    **Response:**
    - Returns success message and updated value
    - Returns 500 if update fails
    
    **Use cases:**
    - Configure n8n webhook
    - Store YouTube cookies
    - Update system settings
    """
    if set_setting(setting_key, request.value):
        return {"message": f"Setting '{setting_key}' updated successfully", "value": request.value}
    else:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to update setting '{setting_key}'"
        )


# Cookie Management Endpoints
class CookieRequest(BaseModel):
    cookies: str = Field(..., description="Cookie string from browser (format: 'name1=value1; name2=value2; ...')")


@app.get("/api/admin/cookies")
async def get_cookies_endpoint(user: str = Depends(authenticate_api_request)):
    """
    **Get YouTube Cookies**
    
    Retrieves stored YouTube cookies used for bypassing bot detection.
    
    **What it does:**
    - Returns stored cookie string from database
    - Indicates if cookies are configured
    
    **Response:**
    - Returns cookies string and has_cookies boolean
    - Cookies are null/empty if not configured
    
    **Use cases:**
    - Check if cookies are configured
    - Display cookie status in UI
    - Verify cookie configuration
    """
    cookies = get_setting("youtube_cookies")
    return {
        "cookies": cookies,
        "has_cookies": bool(cookies)
    }


@app.post("/api/admin/cookies")
async def set_cookies_endpoint(
    request: CookieRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    Set YouTube cookies from browser cookie string.
    
    Cookie format: "name1=value1; name2=value2; ..."
    
    To get cookies:
    1. Open YouTube in browser
    2. Open Developer Tools (F12)
    3. Go to Network tab
    4. Visit any YouTube page
    5. Find a request to youtube.com
    6. Copy the 'cookie' header value
    7. Paste it here
    
    Requires authentication.
    """
    if not request.cookies or not request.cookies.strip():
        raise HTTPException(
            status_code=400,
            detail="Cookies string cannot be empty"
        )
    
    # Validate cookie format (should contain at least one =)
    if '=' not in request.cookies:
        raise HTTPException(
            status_code=400,
            detail="Invalid cookie format. Expected format: 'name1=value1; name2=value2; ...'"
        )
    
    if set_setting("youtube_cookies", request.cookies.strip()):
        return {
            "message": "Cookies saved successfully",
            "cookie_count": len([c for c in request.cookies.split(';') if '=' in c])
        }
    else:
        raise HTTPException(
            status_code=500,
            detail="Failed to save cookies"
        )


@app.delete("/api/admin/cookies")
async def delete_cookies_endpoint(user: str = Depends(authenticate_api_request)):
    """
    **Delete YouTube Cookies**
    
    Removes stored YouTube cookies from the database.
    
    **What it does:**
    - Clears cookie setting from database
    - yt-dlp will no longer use cookies for requests
    - May result in bot detection if cookies were preventing it
    
    **Response:**
    - Returns success message
    - Returns 500 if deletion fails
    
    **Use cases:**
    - Clear expired cookies
    - Reset cookie configuration
    - Troubleshoot cookie issues
    """
    if set_setting("youtube_cookies", ""):
        return {"message": "Cookies deleted successfully"}
    else:
        raise HTTPException(
            status_code=500,
            detail="Failed to delete cookies"
        )


# HTML Video ID Extraction Endpoints
class HtmlExtractionRequest(BaseModel):
    """Request model for extracting video IDs from HTML."""
    html: str = Field(..., description="HTML content from YouTube page")


class HtmlExtractionResponse(BaseModel):
    """Response model for HTML extraction."""
    video_ids: List[str] = Field(..., description="List of extracted video IDs")
    count: int = Field(..., description="Total number of unique video IDs found")
    stored: int = Field(0, description="Number of new video IDs stored in database")
    skipped: int = Field(0, description="Number of video IDs that already existed (skipped)")


def extract_youtube_video_ids(html: str) -> List[str]:
    """
    Extract YouTube video IDs from HTML content.
    
    Matches:
    - /watch?v=VIDEO_ID
    - https://www.youtube.com/watch?v=VIDEO_ID
    - https://i.ytimg.com/vi/VIDEO_ID/hqdefault.jpg
    """
    patterns = [
        r"(?:/watch\?v=)([a-zA-Z0-9_-]{11})",
        r"(?:https?://(?:www\.)?youtube\.com/watch\?v=)([a-zA-Z0-9_-]{11})",
        r"(?:https?://i\.ytimg\.com/vi/)([a-zA-Z0-9_-]{11})",
    ]

    video_ids = set()

    for pattern in patterns:
        matches = re.findall(pattern, html)
        video_ids.update(matches)

    return sorted(list(video_ids))


@app.post("/api/extract-video-ids", response_model=HtmlExtractionResponse)
async def extract_video_ids_endpoint(
    request: HtmlExtractionRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Extract Video IDs from HTML**
    
    Parses HTML content from YouTube pages to extract video IDs and automatically stores them in the database.
    
    **What it does:**
    - Accepts HTML content from any YouTube page
    - Uses regex patterns to find video IDs
    - Extracts unique video IDs from HTML
    - Automatically stores new video IDs in database with "pending" status
    - Skips video IDs that already exist in database
    - Returns list of all extracted IDs with storage statistics
    
    **Supported Patterns:**
    - `/watch?v=VIDEO_ID`
    - `https://www.youtube.com/watch?v=VIDEO_ID`
    - `https://i.ytimg.com/vi/VIDEO_ID/hqdefault.jpg`
    
    **Input:**
    - `html`: HTML content from YouTube page (paste page source)
    
    **Response:**
    - `video_ids`: List of all unique video IDs found
    - `count`: Total number of unique video IDs extracted
    - `stored`: Number of new video IDs stored in database
    - `skipped`: Number of video IDs that already existed (skipped)
    
    **Database Storage:**
    - New videos are stored with status "pending"
    - Duplicate videos (already in database) are skipped
    - Video URLs are automatically generated from IDs
    
    **Use cases:**
    - Extract IDs when channel API fails
    - Parse video IDs from custom pages
    - Extract IDs from search results
    - Alternative method for getting video lists
    - Bulk import videos from HTML source
    """
    if not request.html or not request.html.strip():
        raise HTTPException(
            status_code=400,
            detail="HTML content cannot be empty"
        )
    
    try:
        video_ids = extract_youtube_video_ids(request.html)
        
        # Store extracted video IDs in database with "pending" status
        stored_count = 0
        skipped_count = 0
        
        for video_id in video_ids:
            # Check if video already exists
            existing_video = get_video_record(video_id)
            if not existing_video:
                # Create new record with "pending" status
                video_url = f"https://www.youtube.com/watch?v={video_id}"
                create_video_record(
                    video_id=video_id,
                    video_url=video_url,
                    status="pending"
                )
                stored_count += 1
            else:
                skipped_count += 1
        
        return HtmlExtractionResponse(
            video_ids=video_ids,
            count=len(video_ids),
            stored=stored_count,
            skipped=skipped_count
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to extract video IDs: {str(e)}"
        )


# Transcripts API Endpoint
@app.get("/api/transcripts")
async def get_transcripts_endpoint(
    channel: Optional[str] = None,
    search: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    limit: Optional[int] = None,
    offset: Optional[int] = None,
    user: str = Depends(authenticate_api_request)
):
    """
    **Get Transcripts with Filtering**
    
    Retrieves only videos that have been successfully transcribed, with advanced filtering options.
    
    **What it does:**
    - Returns only videos with transcripts (status = processed or success)
    - Applies filters to narrow down results
    - Supports search within transcript content
    - Includes complete video metadata
    
    **Query Parameters:**
    - `channel`: Filter by exact channel/author name match
    - `search`: Search in video_id, title, channel_name, or transcript content (partial match, case-insensitive)
    - `date_from`: Filter videos updated from this date (format: YYYY-MM-DD)
    - `date_to`: Filter videos updated until this date (format: YYYY-MM-DD)
    - `limit`: Maximum number of results to return
    - `offset`: Number of results to skip (for pagination)
    
    **Response:**
    - Returns total count and list of transcripts
    - Each transcript includes: video_id, video_url, title, channel_name, duration, 
      view_count, upload_date, full transcript text, metadata, timestamps
    
    **Use cases:**
    - Search for specific transcripts
    - Get transcripts from specific channel
    - Export processed transcripts
    - Filter transcripts by date range
    """
    from database import get_db_connection, DB_TYPE, fetch_all
    import json
    
    # Select all fields including metadata, only for processed videos
    query = """SELECT id, video_id, video_url, status, transcript, error_message, 
                      title, duration, view_count, upload_date, channel_name, channel_id, metadata,
                      created_at, updated_at 
               FROM video_transcriptions
               WHERE status IN ('success', 'processed') AND transcript IS NOT NULL AND transcript != ''"""
    params = []
    conditions = []
    
    if channel:
        conditions.append("channel_name = %s" if DB_TYPE == "postgres" else "channel_name = ?")
        params.append(channel)
    
    if search:
        search_pattern = f"%{search}%"
        if DB_TYPE == "postgres":
            conditions.append("(video_id ILIKE %s OR title ILIKE %s OR channel_name ILIKE %s OR transcript ILIKE %s)")
        else:
            conditions.append("(video_id LIKE ? OR title LIKE ? OR channel_name LIKE ? OR transcript LIKE ?)")
        params.extend([search_pattern, search_pattern, search_pattern, search_pattern])
    
    if date_from:
        conditions.append("updated_at >= %s" if DB_TYPE == "postgres" else "updated_at >= ?")
        params.append(date_from)
    
    if date_to:
        conditions.append("updated_at <= %s" if DB_TYPE == "postgres" else "updated_at <= ?")
        params.append(f"{date_to} 23:59:59")
    
    if conditions:
        query += " AND " + " AND ".join(conditions)
    
    query += " ORDER BY updated_at DESC"
    
    if limit:
        query += " LIMIT %s" if DB_TYPE == "postgres" else " LIMIT ?"
        params.append(limit)
        
        if offset:
            query += " OFFSET %s" if DB_TYPE == "postgres" else " OFFSET ?"
            params.append(offset)
    
    with get_db_connection() as conn:
        transcripts = fetch_all(conn, query, tuple(params) if params else None)
    
    # Parse JSON metadata for each transcript
    for transcript in transcripts:
        if transcript.get('metadata'):
            try:
                if isinstance(transcript['metadata'], str):
                    transcript['metadata'] = json.loads(transcript['metadata'])
            except (json.JSONDecodeError, TypeError):
                transcript['metadata'] = None
    
    return {
        "total": len(transcripts),
        "transcripts": transcripts
    }


# OpenAI GPT API Integration
class OpenAIRequest(BaseModel):
    """Request model for OpenAI GPT processing."""
    prompt: Optional[str] = Field(
        None,
        description="The prompt to send to OpenAI GPT model (optional if prompt_id is provided)"
    )
    prompt_id: Optional[int] = Field(
        None,
        description="ID of saved prompt to use (optional if prompt is provided)"
    )
    prompt_variables: Optional[dict] = Field(
        None,
        description="Variables to replace in prompt template (e.g., {'video_title': 'Title', 'transcript': '...'})"
    )
    model: Optional[str] = Field(
        "gpt-3.5-turbo",
        description="OpenAI model to use (default: gpt-3.5-turbo). Options: gpt-3.5-turbo, gpt-4, gpt-4-turbo, gpt-4o-mini"
    )
    temperature: Optional[float] = Field(
        0.7,
        description="Temperature for response generation (0.0 to 2.0, default: 0.7)",
        ge=0.0,
        le=2.0
    )
    max_tokens: Optional[int] = Field(
        1000,
        description="Maximum number of tokens in response (default: 1000)",
        ge=1,
        le=4000
    )
    openai_api_key: Optional[str] = Field(
        None,
        description="OpenAI API key (optional if set via OPENAI_API_KEY environment variable)"
    )


class OpenAIResponse(BaseModel):
    """Response model for OpenAI GPT processing."""
    prompt: str
    model: str
    response: str
    usage: Optional[dict] = None
    error: Optional[str] = None


@app.post("/api/ai/process", response_model=OpenAIResponse)
async def process_with_openai(
    request: OpenAIRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Process Prompt with OpenAI GPT**
    
    Generates AI content using OpenAI's GPT models (GPT-3.5, GPT-4, etc.).
    
    **What it does:**
    1. Accepts a prompt (either direct text or saved prompt ID)
    2. Processes prompt using OpenAI API
    3. Returns generated text response
    4. Includes token usage statistics
    
    **Input Options:**
    - **Direct Prompt**: Provide `prompt` field with your text
    - **Saved Prompt**: Provide `prompt_id` to use a saved prompt template
      - Combines system_prompt and user_prompt_template
      - Supports variable replacement with `prompt_variables`
    
    **Prompt Variables:**
    - Use `{variable_name}` in prompt templates
    - Provide values in `prompt_variables` dict
    - Example: `{'video_title': 'My Video', 'transcript': '...'}`
    
    **Model Options:**
    - `gpt-3.5-turbo`: Fast and cost-effective (default)
    - `gpt-4o-mini`: Balanced performance
    - `gpt-4`: Most capable
    - `gpt-4-turbo`: Latest features
    
    **Parameters:**
    - `temperature`: 0.0 (deterministic) to 2.0 (very creative), default: 0.7
    - `max_tokens`: Maximum response length (1-4000), default: 1000
    - `openai_api_key`: Optional (uses OPENAI_API_KEY env var if not provided)
    
    **Response:**
    - Returns generated text
    - Returns usage statistics (prompt_tokens, completion_tokens, total_tokens)
    - Returns error message if processing fails
    
    **Use cases:**
    - Summarize transcripts
    - Extract keywords
    - Generate content from video data
    - Analyze video content
    """
    # Get OpenAI API key
    api_key = request.openai_api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise HTTPException(
            status_code=400,
            detail="OpenAI API key is required. Provide it in the request or set OPENAI_API_KEY environment variable."
        )
    
    # Get prompt text
    final_prompt = None
    if request.prompt_id:
        # Load saved prompt
        saved_prompt = get_prompt(request.prompt_id)
        if not saved_prompt:
            raise HTTPException(
                status_code=404,
                detail=f"Prompt with ID {request.prompt_id} not found"
            )
        
        # Combine system prompt and user prompt template
        system_prompt = saved_prompt.get('system_prompt') or ''
        user_prompt = saved_prompt.get('user_prompt_template') or ''
        
        # Replace variables in user prompt template
        if request.prompt_variables and user_prompt:
            for key, value in request.prompt_variables.items():
                user_prompt = user_prompt.replace(f"{{{key}}}", str(value))
        
        # Combine prompts
        if system_prompt and user_prompt:
            final_prompt = f"{system_prompt}\n\n{user_prompt}"
        elif system_prompt:
            final_prompt = system_prompt
        elif user_prompt:
            final_prompt = user_prompt
        else:
            raise HTTPException(
                status_code=400,
                detail="Saved prompt has no system_prompt or user_prompt_template"
            )
    elif request.prompt:
        final_prompt = request.prompt
        # Replace variables if provided
        if request.prompt_variables:
            for key, value in request.prompt_variables.items():
                final_prompt = final_prompt.replace(f"{{{key}}}", str(value))
    else:
        raise HTTPException(
            status_code=400,
            detail="Either 'prompt' or 'prompt_id' must be provided"
        )
    
    try:
        # Initialize OpenAI client
        client = OpenAI(api_key=api_key)
        
        # Prepare messages
        messages = []
        # If we have a system prompt (from saved prompt), add it
        if request.prompt_id:
            saved_prompt = get_prompt(request.prompt_id)
            system_prompt = saved_prompt.get('system_prompt') or ''
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
                # User prompt is the user_prompt_template
                user_prompt = saved_prompt.get('user_prompt_template') or ''
                if user_prompt:
                    # Replace variables
                    if request.prompt_variables:
                        for key, value in request.prompt_variables.items():
                            user_prompt = user_prompt.replace(f"{{{key}}}", str(value))
                    messages.append({"role": "user", "content": user_prompt})
                else:
                    messages.append({"role": "user", "content": final_prompt})
            else:
                messages.append({"role": "user", "content": final_prompt})
        else:
            messages.append({"role": "user", "content": final_prompt})
        
        # Call OpenAI API
        response = client.chat.completions.create(
            model=request.model,
            messages=messages,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        # Extract response
        generated_text = response.choices[0].message.content
        
        # Extract usage information
        usage_info = None
        if response.usage:
            usage_info = {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        
        return OpenAIResponse(
            prompt=final_prompt,
            model=request.model,
            response=generated_text,
            usage=usage_info
        )
    
    except Exception as e:
        error_msg = str(e)
        # Return error in response instead of raising exception
        return OpenAIResponse(
            prompt=final_prompt or "",
            model=request.model,
            response="",
            error=error_msg
        )


# Prompt Management Endpoints
class PromptRequest(BaseModel):
    """Request model for creating/updating a prompt."""
    name: str = Field(..., description="Prompt name")
    description: Optional[str] = Field(None, description="Prompt description")
    system_prompt: Optional[str] = Field(None, description="System prompt (instructions for AI)")
    user_prompt_template: Optional[str] = Field(None, description="User prompt template (can contain {variables})")
    operation_type: Optional[str] = Field(None, description="Operation type (e.g., 'summarize', 'extract_keywords', 'generate_content')")


class PromptResponse(BaseModel):
    """Response model for prompt."""
    id: int
    name: str
    description: Optional[str] = None
    system_prompt: Optional[str] = None
    user_prompt_template: Optional[str] = None
    operation_type: Optional[str] = None
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


@app.get("/api/prompts", response_model=List[PromptResponse])
async def list_prompts(
    operation_type: Optional[str] = None,
    user: str = Depends(authenticate_api_request)
):
    """
    **List All Prompts**
    
    Retrieves all saved prompts from the database, optionally filtered by operation type.
    
    **What it does:**
    - Returns all saved prompts with their configurations
    - Can filter by operation_type (e.g., 'summarize', 'extract_keywords')
    - Includes system prompts, user templates, and metadata
    
    **Query Parameters:**
    - `operation_type` (optional): Filter prompts by operation type
    
    **Response:**
    - Returns list of prompts with: id, name, description, system_prompt, 
      user_prompt_template, operation_type, timestamps
    
    **Use cases:**
    - Display prompts in UI dropdown
    - Manage prompt library
    - Filter prompts by category
    """
    prompts = get_all_prompts(operation_type)
    return prompts


@app.get("/api/prompts/{prompt_id}", response_model=PromptResponse)
async def get_prompt_endpoint(
    prompt_id: int,
    user: str = Depends(authenticate_api_request)
):
    """
    **Get Prompt by ID**
    
    Retrieves a specific saved prompt with all its details.
    
    **What it does:**
    - Looks up prompt by ID
    - Returns complete prompt configuration
    - Includes system prompt, user template, and metadata
    
    **Response:**
    - Returns 200 with prompt data if found
    - Returns 404 if prompt not found
    
    **Use cases:**
    - Load prompt for editing
    - Preview prompt before using
    - Get prompt details for API calls
    """
    prompt = get_prompt(prompt_id)
    if not prompt:
        raise HTTPException(status_code=404, detail="Prompt not found")
    return prompt


@app.post("/api/prompts", response_model=PromptResponse)
async def create_prompt_endpoint(
    request: PromptRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Create New Prompt**
    
    Creates a new prompt template for reuse in AI processing.
    
    **What it does:**
    - Stores prompt configuration in database
    - Combines system_prompt (AI instructions) and user_prompt_template (user input)
    - Supports variable placeholders in templates (e.g., {video_title}, {transcript})
    - Categorizes by operation_type for organization
    
    **Required Fields:**
    - `name`: Prompt name (for identification)
    
    **Optional Fields:**
    - `description`: Brief description of what the prompt does
    - `system_prompt`: Instructions for AI (sent as "system" message)
    - `user_prompt_template`: User prompt template with {variables}
    - `operation_type`: Category (e.g., 'summarize', 'extract_keywords')
    
    **Response:**
    - Returns created prompt with assigned ID
    - Includes all prompt fields and timestamps
    
    **Use cases:**
    - Save frequently used prompts
    - Create prompt templates for different operations
    - Build prompt library for team use
    """
    prompt_id = create_prompt(
        name=request.name,
        description=request.description,
        system_prompt=request.system_prompt,
        user_prompt_template=request.user_prompt_template,
        operation_type=request.operation_type
    )
    prompt = get_prompt(prompt_id)
    return prompt


@app.put("/api/prompts/{prompt_id}", response_model=PromptResponse)
async def update_prompt_endpoint(
    prompt_id: int,
    request: PromptRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Update Existing Prompt**
    
    Updates an existing prompt's configuration.
    
    **What it does:**
    - Updates prompt fields (name, description, system_prompt, user_prompt_template, operation_type)
    - Updates timestamp to reflect modification
    - Returns updated prompt data
    
    **Response:**
    - Returns 200 with updated prompt if found
    - Returns 404 if prompt not found
    
    **Use cases:**
    - Edit prompt templates
    - Refine prompts based on results
    - Update prompt categories
    """
    if not get_prompt(prompt_id):
        raise HTTPException(status_code=404, detail="Prompt not found")
    
    update_prompt(
        prompt_id=prompt_id,
        name=request.name,
        description=request.description,
        system_prompt=request.system_prompt,
        user_prompt_template=request.user_prompt_template,
        operation_type=request.operation_type
    )
    prompt = get_prompt(prompt_id)
    return prompt


@app.delete("/api/prompts/{prompt_id}")
async def delete_prompt_endpoint(
    prompt_id: int,
    user: str = Depends(authenticate_api_request)
):
    """
    **Delete Prompt**
    
    Permanently deletes a prompt from the database.
    
    **What it does:**
    - Removes prompt from database
    - Note: Does not delete generated content that used this prompt
    
    **Response:**
    - Returns 200 with success message if deleted
    - Returns 404 if prompt not found
    - Returns 500 if deletion fails
    
    **Use cases:**
    - Remove unused prompts
    - Clean up prompt library
    - Delete test prompts
    """
    if not get_prompt(prompt_id):
        raise HTTPException(status_code=404, detail="Prompt not found")
    
    if delete_prompt(prompt_id):
        return {"message": "Prompt deleted successfully"}
    else:
        raise HTTPException(status_code=500, detail="Failed to delete prompt")


# Bulk Operations Endpoints
class BulkVideoRequest(BaseModel):
    """Request model for bulk video operations."""
    video_ids: List[str] = Field(..., description="List of video IDs to process")
    action: str = Field(..., description="Action to perform: 'transcribe', 'get_data', 'generate_content'")


class BulkTranscribeRequest(BaseModel):
    """Request model for bulk transcription."""
    video_ids: List[str] = Field(..., description="List of video IDs to transcribe")
    deepgram_api_key: Optional[str] = Field(None, description="Deepgram API key (optional if set via env var)")


class BulkGenerateContentRequest(BaseModel):
    """Request model for bulk content generation."""
    video_ids: List[str] = Field(..., description="List of video IDs to generate content for")
    prompt_id: Optional[int] = Field(None, description="ID of saved prompt to use")
    prompt: Optional[str] = Field(None, description="Custom prompt (optional if prompt_id provided)")
    prompt_variables: Optional[dict] = Field(None, description="Variables for prompt template")
    model: Optional[str] = Field("gpt-3.5-turbo", description="OpenAI model to use")
    temperature: Optional[float] = Field(0.7, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(1000, ge=1, le=4000)
    openai_api_key: Optional[str] = Field(None, description="OpenAI API key")


@app.post("/api/bulk/transcribe")
async def bulk_transcribe(
    request: BulkTranscribeRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Bulk Transcribe Videos**
    
    Transcribes multiple videos in a single request.
    
    **What it does:**
    - Accepts list of video IDs
    - Transcribes each video using Deepgram API
    - Returns results for all videos
    - Stores transcripts in database
    
    **Input:**
    - `video_ids`: List of YouTube video IDs to transcribe
    - `deepgram_api_key`: Optional (uses DEEPGRAM_API_KEY env var if not provided)
    
    **Response:**
    - Returns total count and results array
    - Each result includes: video_id, success status, transcript (if successful), error (if failed)
    
    **Use cases:**
    - Process multiple videos at once
    - Batch transcription jobs
    - Retry failed transcriptions
    """
    api_key = request.deepgram_api_key or os.getenv("DEEPGRAM_API_KEY")
    if not api_key:
        raise HTTPException(status_code=400, detail="Deepgram API key is required")
    
    results = []
    for video_id in request.video_ids:
        try:
            # Use existing transcribe logic
            video_request = VideoRequest(videos=[video_id], deepgram_api_key=api_key)
            result = await transcribe_videos(video_request)
            results.append({
                "video_id": video_id,
                "success": len(result.success) > 0,
                "transcript": result.success[0].transcript if result.success else None,
                "error": result.errors[0].error if result.errors else None
            })
        except Exception as e:
            results.append({
                "video_id": video_id,
                "success": False,
                "error": str(e)
            })
    
    return {
        "total": len(request.video_ids),
        "results": results
    }


@app.post("/api/bulk/generate-content")
async def bulk_generate_content(
    request: BulkGenerateContentRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Bulk Generate AI Content for Videos**
    
    Generates AI content for multiple videos using OpenAI GPT models.
    
    **What it does:**
    1. Accepts list of video IDs
    2. For each video:
       - Retrieves video data (title, channel, transcript, etc.)
       - Prepares prompt with video data as variables
       - Calls OpenAI API to generate content
       - Stores generated content in database (1 video â†’ many content items)
    3. Returns results for all videos
    
    **Input:**
    - `video_ids`: List of YouTube video IDs
    - `prompt_id`: Optional - ID of saved prompt to use
    - `prompt`: Optional - Custom prompt text (if not using saved prompt)
    - `prompt_variables`: Optional - Additional variables for prompt template
    - `model`: OpenAI model (default: gpt-3.5-turbo)
    - `temperature`: 0.0-2.0 (default: 0.7)
    - `max_tokens`: 1-4000 (default: 1000)
    - `openai_api_key`: Optional (uses OPENAI_API_KEY env var)
    
    **Automatic Variables:**
    The following variables are automatically injected from video data:
    - `{video_id}`, `{video_title}`, `{channel_name}`, `{transcript}`, `{duration}`, `{view_count}`
    
    **Response:**
    - Returns total count and results array
    - Each result includes: video_id, success status, content (generated text), content_id (database ID), error, usage stats
    - Generated content is stored in database with foreign key to video
    
    **Use cases:**
    - Generate summaries for multiple videos
    - Extract keywords from multiple transcripts
    - Batch content generation
    - Process entire video collections
    """
    results = []
    
    for video_id in request.video_ids:
        try:
            # Get video data
            video_record = get_video_record(video_id)
            if not video_record:
                results.append({
                    "video_id": video_id,
                    "success": False,
                    "error": "Video not found"
                })
                continue
            
            # Prepare prompt variables
            prompt_vars = request.prompt_variables or {}
            prompt_vars.update({
                "video_id": video_id,
                "video_title": video_record.get('title', ''),
                "channel_name": video_record.get('channel_name', ''),
                "transcript": video_record.get('transcript', ''),
                "duration": video_record.get('duration', ''),
                "view_count": video_record.get('view_count', '')
            })
            
            # Create AI request
            ai_request = OpenAIRequest(
                prompt_id=request.prompt_id,
                prompt=request.prompt,
                prompt_variables=prompt_vars,
                model=request.model,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                openai_api_key=request.openai_api_key
            )
            
            # Process with AI
            ai_response = await process_with_openai(ai_request, user)
            
            # Store generated content in database if successful
            content_id = None
            if not ai_response.error and ai_response.response:
                try:
                    # Get prompt text for storage
                    prompt_text = None
                    if request.prompt_id:
                        saved_prompt = get_prompt(request.prompt_id)
                        if saved_prompt:
                            prompt_text = saved_prompt.get('user_prompt_template') or saved_prompt.get('system_prompt')
                    elif request.prompt:
                        prompt_text = request.prompt
                    
                    content_id = create_generated_content(
                        video_id=video_id,
                        generated_text=ai_response.response,
                        prompt_id=request.prompt_id,
                        prompt_text=prompt_text,
                        model=request.model,
                        temperature=request.temperature,
                        max_tokens=request.max_tokens,
                        usage_info=ai_response.usage
                    )
                except Exception as e:
                    print(f"Warning: Failed to store generated content for video {video_id}: {e}")
            
            results.append({
                "video_id": video_id,
                "success": not bool(ai_response.error),
                "content": ai_response.response,
                "content_id": content_id,
                "error": ai_response.error,
                "usage": ai_response.usage
            })
        except Exception as e:
            results.append({
                "video_id": video_id,
                "success": False,
                "error": str(e)
            })
    
    return {
        "total": len(request.video_ids),
        "results": results
    }


@app.post("/api/bulk/get-data")
async def bulk_get_data(
    video_ids: List[str],
    user: str = Depends(authenticate_api_request)
):
    """
    **Bulk Get Video Data**
    
    Retrieves complete data for multiple videos in a single request.
    
    **What it does:**
    - Accepts list of video IDs
    - Retrieves all stored data for each video
    - Returns comprehensive metadata and transcripts
    
    **Input:**
    - `video_ids`: List of YouTube video IDs (in request body as JSON array)
    
    **Response:**
    - Returns total count and results array
    - Each result includes: video_id, title, channel_name, duration, view_count, 
      upload_date, status, transcript, metadata (full JSON)
    - Returns error message if video not found
    
    **Use cases:**
    - Export data for multiple videos
    - Batch data retrieval
    - Data analysis and reporting
    - API integrations requiring bulk data
    """
    results = []
    for video_id in video_ids:
        video_record = get_video_record(video_id)
        if video_record:
            import json
            metadata = video_record.get('metadata')
            if metadata and isinstance(metadata, str):
                try:
                    metadata = json.loads(metadata)
                except:
                    metadata = None
            
            results.append({
                "video_id": video_id,
                "title": video_record.get('title'),
                "channel_name": video_record.get('channel_name'),
                "duration": video_record.get('duration'),
                "view_count": video_record.get('view_count'),
                "upload_date": video_record.get('upload_date'),
                "status": video_record.get('status'),
                "transcript": video_record.get('transcript'),
                "metadata": metadata
            })
        else:
            results.append({
                "video_id": video_id,
                "error": "Video not found"
            })
    
    return {
        "total": len(video_ids),
        "results": results
    }


# Generated Content Endpoints
class GeneratedContentResponse(BaseModel):
    """Response model for generated content."""
    id: int
    video_id: str
    prompt_id: Optional[int] = None
    prompt_text: Optional[str] = None
    model: Optional[str] = None
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    generated_text: str
    usage_info: Optional[dict] = None
    created_at: Optional[str] = None


@app.get("/api/videos/{video_id}/generated-content", response_model=List[GeneratedContentResponse])
async def get_video_generated_content(
    video_id: str,
    user: str = Depends(authenticate_api_request)
):
    """
    **Get All Generated Content for a Video**
    
    Retrieves all AI-generated content items for a specific video (1 video â†’ many content items).
    
    **What it does:**
    - Looks up all generated content records for the video
    - Returns list of all content items ordered by creation date (newest first)
    - Includes complete content text and metadata
    
    **Response:**
    - Returns list of generated content items
    - Each item includes: id, video_id, prompt_id, prompt_text, model, temperature, 
      max_tokens, generated_text (full text), usage_info, created_at
    - Returns empty list if no content found
    
    **Use cases:**
    - View all AI-generated content for a video
    - Compare different generations
    - Export content history
    - Manage content items
    """
    contents = get_generated_content_by_video(video_id)
    return contents


@app.get("/api/generated-content/{content_id}", response_model=GeneratedContentResponse)
async def get_generated_content_endpoint(
    content_id: int,
    user: str = Depends(authenticate_api_request)
):
    """
    **Get Generated Content by ID**
    
    Retrieves a specific generated content item by its database ID.
    
    **What it does:**
    - Looks up generated content by ID
    - Returns complete content data including full generated text
    
    **Response:**
    - Returns 200 with content data if found
    - Returns 404 if content not found
    
    **Use cases:**
    - Retrieve specific content item
    - View content details
    - API integrations requiring specific content
    """
    content = get_generated_content(content_id)
    if not content:
        raise HTTPException(status_code=404, detail="Generated content not found")
    return content


@app.get("/api/generated-content", response_model=List[GeneratedContentResponse])
async def list_all_generated_content(
    limit: Optional[int] = None,
    offset: Optional[int] = None,
    user: str = Depends(authenticate_api_request)
):
    """
    **List All Generated Content**
    
    Retrieves all generated content items across all videos with pagination support.
    
    **What it does:**
    - Returns all generated content from database
    - Ordered by creation date (newest first)
    - Supports pagination for large datasets
    
    **Query Parameters:**
    - `limit`: Maximum number of results to return
    - `offset`: Number of results to skip (for pagination)
    
    **Response:**
    - Returns list of all generated content items
    - Each item includes complete data: id, video_id, prompt info, generated text, usage stats
    
    **Use cases:**
    - Browse all generated content
    - Export all content
    - Analytics and reporting
    - Content management
    """
    contents = get_all_generated_content(limit=limit, offset=offset)
    return contents


@app.delete("/api/generated-content/{content_id}")
async def delete_generated_content_endpoint(
    content_id: int,
    user: str = Depends(authenticate_api_request)
):
    """
    **Delete Generated Content**
    
    Permanently deletes a generated content item from the database.
    
    **What it does:**
    - Removes generated content record by ID
    - Does not affect the associated video
    - Permanent deletion (cannot be undone)
    
    **Response:**
    - Returns 200 with success message if deleted
    - Returns 404 if content not found
    - Returns 500 if deletion fails
    
    **Use cases:**
    - Remove unwanted content
    - Clean up test content
    - Manage content storage
    """
    if not get_generated_content(content_id):
        raise HTTPException(status_code=404, detail="Generated content not found")
    
    if delete_generated_content(content_id):
        return {"message": "Generated content deleted successfully"}
    else:
        raise HTTPException(status_code=500, detail="Failed to delete generated content")


# Video Ignore Management Endpoints
class BulkIgnoreVideoRequest(BaseModel):
    video_ids: List[str]
    ignored: bool


@app.post("/api/videos/{video_id}/ignore")
async def ignore_video_endpoint(
    video_id: str,
    ignored: bool = True,
    user: str = Depends(authenticate_api_request)
):
    """
    **Mark Video as Ignored/Active**
    
    Marks a video as ignored (hidden from default views) or active.
    
    **What it does:**
    - Sets ignored status for a video
    - Ignored videos are hidden by default in video listings
    - Use show_ignored=true parameter to view ignored videos
    
    **Query Parameters:**
    - `ignored`: true to ignore, false to activate (default: true)
    
    **Response:**
    - Returns success message
    - Returns 404 if video not found
    """
    if update_video_ignored_status(video_id, ignored):
        status = "ignored" if ignored else "activated"
        return {"message": f"Video {status} successfully"}
    else:
        raise HTTPException(status_code=404, detail="Video not found")


@app.post("/api/videos/bulk-ignore")
async def bulk_ignore_videos_endpoint(
    request: BulkIgnoreVideoRequest,
    user: str = Depends(authenticate_api_request)
):
    """
    **Bulk Mark Videos as Ignored/Active**
    
    Marks multiple videos as ignored or active in a single request.
    
    **Request:**
    - `video_ids`: List of video IDs to update
    - `ignored`: true to ignore, false to activate
    
    **Response:**
    - Returns count of updated videos
    """
    count = bulk_update_video_ignored_status(request.video_ids, request.ignored)
    status = "ignored" if request.ignored else "activated"
    return {
        "message": f"{count} video(s) {status} successfully",
        "count": count
    }
